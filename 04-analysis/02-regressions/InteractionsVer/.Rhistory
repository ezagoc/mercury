knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
df <- read_parquet("./data/6-labeled/training_true_final.parquet")
rm(list = ls())
library(tidyverse)
library(tm)
library(arrow)
library(topicmodels)
library(arules)
library(tidytext)
df <- read_parquet("./data/6-labeled/training_true_final.parquet")
df_fake <- read_parquet("./data/6-labeled/training_fake_final.parquet")
# Functions:
clean_text <- function(df){
df <- df |> mutate(text_clean = gsub("[^\\s]*https://[^\\s]*","", text, perl=T),
text_clean = str_replace_all(text_clean, "[:punct:]", ""),
text_clean = tolower(text_clean),
text_clean = gsub('[[:digit:]]+', '', text_clean),
text_clean = removeWords(text_clean, stopwords("en")),
text_clean = trimws(text_clean, "l"),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T))
df
}
# Cleaning the data:
df <- clean_text(df)
df_fake <- clean_text(df_fake)
# Function to run the LDA and generate the graph:
lda_frame <- function(document, topics, semilla = 1234){
ap_lda <- LDA(document, k = topics, control = list(seed = semilla))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
}
lda_graph <- function(df){
ap_top_terms <- df %>% group_by(topic) %>% slice_max(beta, n = 10) %>%
ungroup() %>% arrange(topic, -beta)
graph <- ap_top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = "free") +
scale_y_reordered() + ggtitle("Most Common Terms within each Topic") +
labs(x = "Beta", y = "Terms") + theme_bw()
graph
}
library(topicdoc)
run_diags <- function(document, topics, semilla = 1234){
ap_lda <- LDA(document, k = topics, control = list(seed = semilla))
df <- topic_diagnostics(ap_lda, tdm) |>
select(topic_num, topic_exclusivity, topic_size)
df <- df %>%
mutate(topic_label = terms(ap_lda, topics) %>%
apply(2, paste, collapse = ", "),
topic_label = paste(topic_num, topic_label, sep = " - "))
graph <- df %>%
gather(diagnostic, value, -topic_label, -topic_num) %>%
ggplot(aes(x = topic_num, y = value, fill = str_wrap(topic_label, 25))) +
geom_bar(stat = "identity") + facet_wrap(~diagnostic, scales = "free") +
labs(x = "Topic Number", y = "Diagnostic Value",
fill = "Topic Label", title = paste0("Topic Exclusivity and Size: ", topics, "-Topic Model"))
graph
}
extra_stops_fake <- c("will", "us", "one", "two", "can", "now", "us", "also",
"also")
df_fake <- df_fake |>
mutate(text_clean = removeWords(text_clean, extra_stops_fake))
myCorpus_fake <- Corpus(VectorSource(df_fake$text_clean))
tdm <- DocumentTermMatrix(myCorpus_fake)
ap9 <- lda_frame(tdm, 5)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 9)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 7)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 7)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 17, height = 15, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 9)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 19, height = 17, units = 'cm') # Salvamos en pdf
packages <- c(
"foreign",
"dplyr",
"tidyverse",
"blockTools",
"writexl",
"lmtest",
"sandwich"
)
library(nbpMatching)
library(arrow)
ipak(packages)
load("SA/03-assignment/input/twitter_followers.Rda")
setwd("../../data/02-randomize/")
setwd("../../data/02-randomize/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
library(academictwitteR)
library(tidyverse)
bearer_tok = 'AAAAAAAAAAAAAAAAAAAAAB%2F8lAEAAAAAk3oQmU9uoo6MZGtW5RmzPkS8oRc%3DT4T24oiYAFmLW76FXoqshmU99fKtVpIWWS2AuQ3r7LcXyzb5oR'
# Getting user id
id <- get_user_id('lalozcc', bearer_token = bearer_tok)
followers <- get_user_followers(id, bearer_token = bearer_tok)
View(followers)
followers_info <- get_user_profile('lalozcc', bearer_token = bearer_tok)
followers_info <- get_user_profile(id, bearer_token = bearer_tok)
View(followers_info)
View(followers)
View(followers[[2]][[1]])
View(followers[[2]][[2]])
View(followers)
followers_list <- followers |> select(id) |> as_vector()
followers_info <- get_user_profile(followers_list, bearer_token = bearer_tok)
View(followers_info)
reticulate::repl_python()
import pandas as pd
import os
import glob
import numpy as np
import time
from os import listdir
from os.path import isfile, join
from tqdm import tqdm
library(arrow)
library(academictwitteR)
library(tidyverse)
setwd("../../data/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/")
setwd("../../data/verifiable/")
setwd("../../data/verifiable/")
setwd("../../data/verifiable/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/verifiable/")
data <- read_parquet('../../data/04-analysis/KE/baseline_features.parquet')
rm(list = ls())
src_path <- c("../../src/utils/")
source_files <- list(
"funcs.R",
"constants_balance.R"
)
map(paste0(src_path, source_files), source)
map(paste0(src_path, source_files), source)
library("purrr")
map(paste0(src_path, source_files), source)
rm(list = ls())
library("purrr")
src_path <- c("../../src/utils/")
source_files <- list(
"funcs.R",
"constants_balance.R"
)
map(paste0(src_path, source_files), source)
library(academictwitteR)
library(tidyverse)
bearer = 'AAAAAAAAAAAAAAAAAAAAAFsqeAEAAAAAxJhMuhDNgVBlTWIMrgjo4HA8w5U%3DcZXthpBX0nHKyXp6tFdWi7ODKLlAX9dOuTQcN7o9P0dkfQp5a2'
tweets2 <-
get_all_tweets(query = '(feminazi OR radfem OR #feminazi OR #radfem) (lang:es)',
start_tweets = '2020-03-05T00:00:00Z',
end_tweets = '2020-03-15T00:00:00Z',
bearer_token = bearer,
bind_tweets = TRUE,
data_path = "../../data/back_up/hate_speech/2020/",
n= 20000
)
tweets2 <-
get_all_tweets(query = '(feminazi OR radfem OR #feminazi OR #radfem) (lang:es)',
start_tweets = '2020-03-05T00:00:00Z',
end_tweets = '2020-03-15T00:00:00Z',
bearer_token = bearer,
bind_tweets = TRUE,
data_path = "../../data/back_up/hate_speech/2020/",
n= 20000
)
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
install.packages('sf', type = 'source')
library(sf)
library(tidyverse)
shape <- st_read(system.file("shape/nc.shp", package="sf")) # included with sf package
View(shape)
# 3 semi random NC cities
cities <- data.frame(name = c("Raleigh", "Greensboro", "Wilmington"),
x = c(-78.633333, -79.819444, -77.912222),
y = c(35.766667, 36.08, 34.223333)) %>%
st_as_sf(coords = c("x", "y"), crs = 4326) %>%
st_transform(st_crs(shape)) # align coordinate systems
View(cities)
# here be the action!
cities_with_counties <- st_join(cities,
shape,
left = F) %>% # not left = inner join
select(name, county = NAME)
View(cities_with_counties)
df <- arrow::read_parquet(paste0('../../data/01-preprocess/geolocated/', batch,
'/final/users_muns.parquet'))
batch <- 'batch1'
df <- arrow::read_parquet(paste0('../../data/01-preprocess/geolocated/', batch,
'/final/users_muns.parquet'))
sxsw <- read_parquet('../../data/01-preprocess/geolocated/sxsw/integrate/user_coordinates_filtered.parquet') |>
select(username, name, location, created_at, id, description,
loc1:exclude_states)
library(sf)
library(tidyverse)
library(arrow)
library(foreign)
# 0.2 Importing the Data
# Municipality Data:
muns_pol2 <- read_sf('../../data/01-preprocess/shapefiles1/00mun.shp')
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
# Loading the data frames:
########## Municipality data:
muns_pol2 <- read_sf('../../../data/01-preprocess/shapefiles1/00mun.shp')
library(stars)
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
library(arrow)
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
library(arrow)
library(stars)
library(automap)
########## Municipality data:
muns_pol2 <- read_sf('../../../data/01-preprocess/shapefiles1/00mun.shp')
library(academictwitteR)
bearer <- "AAAAAAAAAAAAAAAAAAAAAEEXcQEAAAAANiZf0Y2n%2B2nT%2FCfdCz0kqkpep%2Fs%3Dy0yeu7cSyURT6fVUr1MbobyC57R6aLbHUkwzWRCMJERJhwVf1u"
df1 <- hydrate_tweets('1609488636738957312', bearer_token = bearer)
df2 <- hydrate_tweets('1608908347457277952', bearer_token = bearer)
View(df2)
View(df1)
asinh(0)
# Script to Run all codes from this folder:
# 0.0 Set up the environment, clean it and set working directory to the code path
rm(list = ls())
rstudioapi::getActiveDocumentContext
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# 1.0 Import functions and packages
library(purrr)
src_path <- c("../../../../src/utils/")
source_files <- list(
"funcs.R",
"constants_final.R",
"import_data.R"
)
map(paste0(src_path, source_files), source)
ipak(packages)
`%!in%` = Negate(`%in%`)
# 2.0 Define constants (DO NOT CHANGE)
country <- 'joint'
list_stages <- list('stage1_2', 'stage3_4', 'stage5_6')
list_types <- list( 'log_')
ini <- '../../../../data/04-analysis/joint/'
# 2.1 Change from code to code
data_type <- 'InteractionsVer'
file_code <- 'reactions_ads'
final1 <- tibble()
for (stage in list_stages){
# 3.0 Import data and manipulate
df <- get_analysis_int_ver_final_winsor(stage = stage, batches = 'b1b2',
initial_path = '../../../../')
df <- df |> filter(n_posts_base>0)
df <- df |> mutate(strat_block1 = paste0(strat_block1, batch_id, pais),
strat_block2 = paste0(strat_block2, batch_id, pais))
df_int <- tibble()
for (type in list_types){
aux_aux <- aux_int_reac
aux <- paste0(type, aux_aux)
# Get the Robust S.E.s
se <- c()
coef <- c()
count2 <- 1
for (x in aux) {
fmla1 <- as.formula(paste0(x, "~ ads_treatment + ", x, "_base  | strat_block1"))
nam1 <- paste("lm_", count2, "_se", sep = "")
assign(nam1, feols(fmla1, vcov = 'HC1', data = df))
se <- c(se, get(nam1, envir = globalenv())$se['ads_treatment'])
coef <- c(coef, get(nam1, envir = globalenv())$coefficients['ads_treatment'])
count2 <- count2 + 1
}
df_coef <- tibble(coef = coef, sd = se, var = aux_aux) |>
mutate(stage = stage, type = type)
df_int <- rbind(df_int, df_coef)
}
final1 <- rbind(final1, df_int)
print(stage)
}
for (type in list_types){
if (type == 'log_'){
addon <- 'log '
final <- final1 |> filter(type == 'log_')
} else if(type == 'arc_'){
addon <- 'arcsinh '
final <- final1 |> filter(type == 'arc_')
}else {
addon <- ''
final <- final1 |> filter(type == '')
}
final <- final |>
mutate(Variable = case_when(var == aux_aux[1] ~ paste0(addon, 'Total Reactions (Verifiable)'),
var == aux_aux[2] ~ paste0(addon, 'Total Reactions (True)'),
var == aux_aux[3] ~ paste0(addon, 'Total Reactions (Fake)'),
var == aux_aux[4] ~ paste0(addon, 'Total Reactions (English)'),
var == aux_aux[5] ~ paste0(addon,
'Total Reactions (Non-Verifiable)')),
Stage = case_when(stage == 'stage1_2' ~ 'Weeks 1-4',
stage == 'stage3_4' ~ 'Weeks 5-8',
stage == 'stage5_6' ~ 'Weeks 9-12'))
final$Variable <- factor(final$Variable, levels = c(paste0(addon, 'Total Reactions (English)'),
paste0(addon,
'Total Reactions (Non-Verifiable)'),
paste0(addon, 'Total Reactions (Verifiable)'),
paste0(addon, 'Total Reactions (True)'),
paste0(addon, 'Total Reactions (Fake)')))
results_plot <- ggplot(data = final, aes(x = factor(Stage), y = coef)) +
geom_point(aes(shape = factor(Variable), color = factor(Variable)), size = 3,
position = position_dodge(width = 0.5)) +
geom_linerange(aes(ymin = coef - 1.96 * sd, ymax = coef + 1.96 * sd,
color = factor(Variable)),
position = position_dodge(width = 0.5), size = 1) +
scale_shape_manual(values = c(15, 16, 17, 4, 7), name = 'Outcome') +
scale_color_manual(values = rep('black', 5), name = 'Outcome') +
geom_hline(yintercept = 0, linetype = "solid", color = "black", size = .5) +  # Set custom fill colors for points # Set custom line colors for error bars
theme_bw() +
ylab("Treated Estimate with 95% Confidence Interval") +
xlab("Stage") +  # Change title color
#ggtitle("Dynamic Effects of the Intervention: Verifiability Analysis") +
theme(panel.grid.major = element_line(color = "gray", linetype = "dashed", size = 0.5),
panel.grid.minor = element_blank(),
axis.text.x = element_text(angle = 45, hjust = 1))
ggsave(results_plot,
filename = paste0('../../../../results/01-regression_graphs/',
data_type, '/', type, file_code, '.pdf'),
device = cairo_pdf, width = 8.22, height = 6.59, units = 'in')
}
results_plot
