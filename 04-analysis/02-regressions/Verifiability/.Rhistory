knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
df <- read_parquet("./data/6-labeled/training_true_final.parquet")
rm(list = ls())
library(tidyverse)
library(tm)
library(arrow)
library(topicmodels)
library(arules)
library(tidytext)
df <- read_parquet("./data/6-labeled/training_true_final.parquet")
df_fake <- read_parquet("./data/6-labeled/training_fake_final.parquet")
# Functions:
clean_text <- function(df){
df <- df |> mutate(text_clean = gsub("[^\\s]*https://[^\\s]*","", text, perl=T),
text_clean = str_replace_all(text_clean, "[:punct:]", ""),
text_clean = tolower(text_clean),
text_clean = gsub('[[:digit:]]+', '', text_clean),
text_clean = removeWords(text_clean, stopwords("en")),
text_clean = trimws(text_clean, "l"),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T),
text_clean = gsub("  "," ", text_clean, perl=T))
df
}
# Cleaning the data:
df <- clean_text(df)
df_fake <- clean_text(df_fake)
# Function to run the LDA and generate the graph:
lda_frame <- function(document, topics, semilla = 1234){
ap_lda <- LDA(document, k = topics, control = list(seed = semilla))
ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
}
lda_graph <- function(df){
ap_top_terms <- df %>% group_by(topic) %>% slice_max(beta, n = 10) %>%
ungroup() %>% arrange(topic, -beta)
graph <- ap_top_terms %>% mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = "free") +
scale_y_reordered() + ggtitle("Most Common Terms within each Topic") +
labs(x = "Beta", y = "Terms") + theme_bw()
graph
}
library(topicdoc)
run_diags <- function(document, topics, semilla = 1234){
ap_lda <- LDA(document, k = topics, control = list(seed = semilla))
df <- topic_diagnostics(ap_lda, tdm) |>
select(topic_num, topic_exclusivity, topic_size)
df <- df %>%
mutate(topic_label = terms(ap_lda, topics) %>%
apply(2, paste, collapse = ", "),
topic_label = paste(topic_num, topic_label, sep = " - "))
graph <- df %>%
gather(diagnostic, value, -topic_label, -topic_num) %>%
ggplot(aes(x = topic_num, y = value, fill = str_wrap(topic_label, 25))) +
geom_bar(stat = "identity") + facet_wrap(~diagnostic, scales = "free") +
labs(x = "Topic Number", y = "Diagnostic Value",
fill = "Topic Label", title = paste0("Topic Exclusivity and Size: ", topics, "-Topic Model"))
graph
}
extra_stops_fake <- c("will", "us", "one", "two", "can", "now", "us", "also",
"also")
df_fake <- df_fake |>
mutate(text_clean = removeWords(text_clean, extra_stops_fake))
myCorpus_fake <- Corpus(VectorSource(df_fake$text_clean))
tdm <- DocumentTermMatrix(myCorpus_fake)
ap9 <- lda_frame(tdm, 5)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 9)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 7)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 12, height = 10, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 7)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 17, height = 15, units = 'cm') # Salvamos en pdf
ap9 <- lda_frame(tdm, 9)
g2 <- lda_graph(ap9)
ggsave(g2, filename = '9-topic_model.pdf', device = cairo_pdf,
dpi = 300, width = 19, height = 17, units = 'cm') # Salvamos en pdf
packages <- c(
"foreign",
"dplyr",
"tidyverse",
"blockTools",
"writexl",
"lmtest",
"sandwich"
)
library(nbpMatching)
library(arrow)
ipak(packages)
load("SA/03-assignment/input/twitter_followers.Rda")
setwd("../../data/02-randomize/")
setwd("../../data/02-randomize/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
library(academictwitteR)
library(tidyverse)
bearer_tok = 'AAAAAAAAAAAAAAAAAAAAAB%2F8lAEAAAAAk3oQmU9uoo6MZGtW5RmzPkS8oRc%3DT4T24oiYAFmLW76FXoqshmU99fKtVpIWWS2AuQ3r7LcXyzb5oR'
# Getting user id
id <- get_user_id('lalozcc', bearer_token = bearer_tok)
followers <- get_user_followers(id, bearer_token = bearer_tok)
View(followers)
followers_info <- get_user_profile('lalozcc', bearer_token = bearer_tok)
followers_info <- get_user_profile(id, bearer_token = bearer_tok)
View(followers_info)
View(followers)
View(followers[[2]][[1]])
View(followers[[2]][[2]])
View(followers)
followers_list <- followers |> select(id) |> as_vector()
followers_info <- get_user_profile(followers_list, bearer_token = bearer_tok)
View(followers_info)
reticulate::repl_python()
import pandas as pd
import os
import glob
import numpy as np
import time
from os import listdir
from os.path import isfile, join
from tqdm import tqdm
library(arrow)
library(academictwitteR)
library(tidyverse)
setwd("../../data/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/")
setwd("../../data/verifiable/")
setwd("../../data/verifiable/")
setwd("../../data/verifiable/")
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/verifiable/")
data <- read_parquet('../../data/04-analysis/KE/baseline_features.parquet')
rm(list = ls())
src_path <- c("../../src/utils/")
source_files <- list(
"funcs.R",
"constants_balance.R"
)
map(paste0(src_path, source_files), source)
map(paste0(src_path, source_files), source)
library("purrr")
map(paste0(src_path, source_files), source)
rm(list = ls())
library("purrr")
src_path <- c("../../src/utils/")
source_files <- list(
"funcs.R",
"constants_balance.R"
)
map(paste0(src_path, source_files), source)
library(academictwitteR)
library(tidyverse)
bearer = 'AAAAAAAAAAAAAAAAAAAAAFsqeAEAAAAAxJhMuhDNgVBlTWIMrgjo4HA8w5U%3DcZXthpBX0nHKyXp6tFdWi7ODKLlAX9dOuTQcN7o9P0dkfQp5a2'
tweets2 <-
get_all_tweets(query = '(feminazi OR radfem OR #feminazi OR #radfem) (lang:es)',
start_tweets = '2020-03-05T00:00:00Z',
end_tweets = '2020-03-15T00:00:00Z',
bearer_token = bearer,
bind_tweets = TRUE,
data_path = "../../data/back_up/hate_speech/2020/",
n= 20000
)
tweets2 <-
get_all_tweets(query = '(feminazi OR radfem OR #feminazi OR #radfem) (lang:es)',
start_tweets = '2020-03-05T00:00:00Z',
end_tweets = '2020-03-15T00:00:00Z',
bearer_token = bearer,
bind_tweets = TRUE,
data_path = "../../data/back_up/hate_speech/2020/",
n= 20000
)
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
install.packages('sf', type = 'source')
library(sf)
library(tidyverse)
shape <- st_read(system.file("shape/nc.shp", package="sf")) # included with sf package
View(shape)
# 3 semi random NC cities
cities <- data.frame(name = c("Raleigh", "Greensboro", "Wilmington"),
x = c(-78.633333, -79.819444, -77.912222),
y = c(35.766667, 36.08, 34.223333)) %>%
st_as_sf(coords = c("x", "y"), crs = 4326) %>%
st_transform(st_crs(shape)) # align coordinate systems
View(cities)
# here be the action!
cities_with_counties <- st_join(cities,
shape,
left = F) %>% # not left = inner join
select(name, county = NAME)
View(cities_with_counties)
df <- arrow::read_parquet(paste0('../../data/01-preprocess/geolocated/', batch,
'/final/users_muns.parquet'))
batch <- 'batch1'
df <- arrow::read_parquet(paste0('../../data/01-preprocess/geolocated/', batch,
'/final/users_muns.parquet'))
sxsw <- read_parquet('../../data/01-preprocess/geolocated/sxsw/integrate/user_coordinates_filtered.parquet') |>
select(username, name, location, created_at, id, description,
loc1:exclude_states)
library(sf)
library(tidyverse)
library(arrow)
library(foreign)
# 0.2 Importing the Data
# Municipality Data:
muns_pol2 <- read_sf('../../data/01-preprocess/shapefiles1/00mun.shp')
rm(list = ls()) # Clean out elements in R environment
setwd("../../data/02-randomize/")
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
# Loading the data frames:
########## Municipality data:
muns_pol2 <- read_sf('../../../data/01-preprocess/shapefiles1/00mun.shp')
library(stars)
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
library(arrow)
library(sp)
library(sf)
library(tidyverse)
library(rgdal)
library(dismo)
library(gstat)
library(rgeos)
library(arrow)
library(stars)
library(automap)
########## Municipality data:
muns_pol2 <- read_sf('../../../data/01-preprocess/shapefiles1/00mun.shp')
library(academictwitteR)
bearer <- "AAAAAAAAAAAAAAAAAAAAAEEXcQEAAAAANiZf0Y2n%2B2nT%2FCfdCz0kqkpep%2Fs%3Dy0yeu7cSyURT6fVUr1MbobyC57R6aLbHUkwzWRCMJERJhwVf1u"
df1 <- hydrate_tweets('1609488636738957312', bearer_token = bearer)
df2 <- hydrate_tweets('1608908347457277952', bearer_token = bearer)
View(df2)
View(df1)
asinh(0)
# 0.0 Set up the environment, clean it and set working directory to the code path
rm(list = ls())
rstudioapi::getActiveDocumentContext
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# 1.0 Import functions and packages
library(purrr)
src_path <- c("../../../../src/utils/")
source_files <- list(
"funcs.R",
"constants_final.R",
"import_data.R"
)
map(paste0(src_path, source_files), source)
ipak(packages)
`%!in%` = Negate(`%in%`)
# 2.0 Define constants
country <- 'joint'
data_type <- 'Verifiability'
list_stages <- list('stage1_2', 'stage3_4', 'stage5_6')
ini <- '../../../../data/04-analysis/joint/'
final1 <- tibble()
for (stage in list_stages){
# 3.0 Import data and manipulate
df <- get_analysis_ver_final_winsor(stage = stage, batches = 'b1b2',
initial_path = '../../../../')
df <- df |> select(id, n_posts_base, n_posts) |>
mutate(n_posts_f = n_posts + n_posts_base)
agg <- df |> group_by(pais, batch_id) |>
summarise(across(c(n_posts_f), ~sum(.x)))
final1 <- rbind(final1, agg)
}
for (stage in list_stages){
# 3.0 Import data and manipulate
df <- get_analysis_ver_final_winsor(stage = stage, batches = 'b1b2',
initial_path = '../../../../')
df <- df |> select(id, n_posts_base, n_posts, pais, batch_id) |>
mutate(n_posts_f = n_posts + n_posts_base)
agg <- df |> group_by(pais, batch_id) |>
summarise(across(c(n_posts_f), ~sum(.x)))
final1 <- rbind(final1, agg)
}
View(final1)
writexl::write_xlsx(final1, 'aggregate_n_posts.xlsx')
