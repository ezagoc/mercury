{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5484a9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from langdetect import DetectorFactory, detect\n",
    "DetectorFactory.seed = 0\n",
    "import re\n",
    "\n",
    "def get_path(country, week):\n",
    "    base = f'../../data/03-experiment/{country}/'\n",
    "    path_tw = base + f'treatment/followers/00-raw/tweets/{week}/'\n",
    "    rand = f'../../data/02-randomize/{country}/04-stratification/integrate/followers_randomized.parquet'\n",
    "    baseline = base + 'baseline/00-raw/followers/tweets/'\n",
    "    agg = base + f'treatment/followers/01-preprocess/'\n",
    "    agg_base = base + 'baseline/01-preprocess/followers/'\n",
    "    return path_tw, base, rand, baseline, agg, agg_base\n",
    "\n",
    "df_vars = ['id', 'handle', 'author_id', 'created_at', 'text', 'lang', 'referenced_tweets',\n",
    "           'entities.urls','public_metrics.like_count', 'public_metrics.quote_count', \n",
    "           'public_metrics.reply_count', 'public_metrics.retweet_count']\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from funcs import PreprocessTweets\n",
    "def compute_engagement_tw(engage):\n",
    "    engage[\"total_reactions\"] = engage[\n",
    "        [x for x in engage.columns if \"public_metrics\" in x]\n",
    "    ].sum(axis=\"columns\")\n",
    "    engage[\"total_comments\"] = engage[\n",
    "        [\"public_metrics.reply_count\", \"public_metrics.quote_count\"]\n",
    "    ].sum(axis=\"columns\")\n",
    "    engage.rename(\n",
    "        {\"public_metrics.retweet_count\": \"total_shares\"}, axis=1, inplace=True\n",
    "    )\n",
    "    return(engage)\n",
    "\n",
    "def lang_detect_na(tweet):\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "    except:\n",
    "        lang = 'NA'\n",
    "    return lang\n",
    "\n",
    "def compute_engagement_tw2(engage):\n",
    "    engage[\"total_reactions\"] = engage[\n",
    "        [\"reply_count\", \"quote_count\", \"like_count\", \n",
    "         'retweet_count']].sum(axis=\"columns\")\n",
    "    engage[\"total_comments\"] = engage[\n",
    "        [\"reply_count\", \n",
    "         \"quote_count\"]].sum(axis=\"columns\")\n",
    "    engage.rename(\n",
    "        {\"retweet_count\": \"total_shares\"}, axis=1, inplace=True)\n",
    "    return(engage)\n",
    "\n",
    "def has_items_tw(df):\n",
    "\n",
    "    has_items = PreprocessTweets(df).preprocess()\n",
    "    has_items.drop(['entities.urls', 'description', 'display_url',\n",
    "    'end', 'expanded_url', 'images', 'media_key', 'start', \n",
    "    'status', 'title', 'unwound_url', 'url'], axis=1, inplace=True)\n",
    "    \n",
    "    return(has_items)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bc65210",
   "metadata": {},
   "source": [
    "### Aggregating baseline posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1696304",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: '../../data/03-experiment/KE/baseline/00-raw/followers/tweets/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m### \u001b[39;00m\n\u001b[0;32m      2\u001b[0m path_tw, base, rand, baseline, agg, agg_base \u001b[39m=\u001b[39m get_path(\u001b[39m'\u001b[39m\u001b[39mKE\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdecember1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m onlyfiles \u001b[39m=\u001b[39m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m listdir(baseline) \u001b[39mif\u001b[39;00m isfile(join(baseline, f))]\n\u001b[0;32m      4\u001b[0m onlyfiles \u001b[39m=\u001b[39m [f\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m.parquet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m onlyfiles]\n\u001b[0;32m      5\u001b[0m onlyfiles\u001b[39m.\u001b[39msort()\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: '../../data/03-experiment/KE/baseline/00-raw/followers/tweets/'"
     ]
    }
   ],
   "source": [
    "### \n",
    "path_tw, base, rand, baseline, agg, agg_base = get_path('KE', 'december1')\n",
    "onlyfiles = [f for f in listdir(baseline) if isfile(join(baseline, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a590bc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|█████████████████████████▏                                                     | 319/1000 [00:12<00:38, 17.83it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(0, 83):\n",
    "    df_final = pd.DataFrame()\n",
    "    time.sleep(5)\n",
    "    a = 1000*i \n",
    "    b = 1000*(i+1)\n",
    "    for e in tqdm(onlyfiles[a:b]):\n",
    "        df = pd.read_parquet(f'{baseline}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final[df_vars]\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg_base}intermediate/baseline_{i}.parquet.gzip', compression = 'gzip')\n",
    "    \n",
    "df_final = pd.DataFrame()\n",
    "for i in tqdm(onlyfiles[82000:]):\n",
    "    df = pd.read_parquet(f'{baseline}{i}.parquet')\n",
    "    df_final = pd.concat([df_final, df])\n",
    "\n",
    "df_final = df_final[df_vars]\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_final.to_parquet(f'{agg_base}intermediate/baseline_83.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4230176c",
   "metadata": {},
   "source": [
    "### SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e0e1772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36803"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tw, base, rand, baseline, agg, agg_base = get_path('SA', 'december1')\n",
    "onlyfiles = [f for f in listdir(baseline) if isfile(join(baseline, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a02277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 179/500 [00:26<00:50,  6.35it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(0, 74):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 500*i \n",
    "    b = 500*(i+1)\n",
    "    time.sleep(5)\n",
    "    for e in tqdm(onlyfiles[a:b]):\n",
    "        time.sleep(0.1)\n",
    "        df = pd.read_parquet(f'{baseline}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final[df_vars]\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = compute_engagement_tw(df_final)\n",
    "    df_final = has_items_tw(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg_base}intermediate/baseline_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc277df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87/87 [00:00<00:00, 96.83it/s] \n"
     ]
    }
   ],
   "source": [
    "df_final = pd.DataFrame()\n",
    "for i in tqdm(onlyfiles[10000:]):\n",
    "    df = pd.read_parquet(f'{baseline2}{i}.parquet')\n",
    "    df = df[df['created_at'] > '2022-08-03T00:00:00Z'].reset_index(drop=True)\n",
    "    df_final = pd.concat([df_final, df])\n",
    "\n",
    "df_final = df_final[df_vars]\n",
    "df_final = compute_engagement_tw(df_final)\n",
    "df_final = has_items_tw(df_final)\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "df_final.to_parquet(f'{agg_base}intermediate_abs/baseline_25.parquet.gzip', compression = 'gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cab347a",
   "metadata": {},
   "source": [
    "### Aggregate Treatment Posts (TweetPle Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d01b812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38632"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tw, base, rand, baseline, agg, agg_base = get_path('KE', 'april1')\n",
    "onlyfiles = [f for f in listdir(path_tw) if isfile(join(path_tw, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e1134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(51, 59):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 1000*i \n",
    "    b = 1000*(i+1)\n",
    "    time.sleep(5)\n",
    "    for e in tqdm(onlyfiles[a:b]):\n",
    "        df = pd.read_parquet(f'{path_tw}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final[df_vars]\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = compute_engagement_tw(df_final)\n",
    "    df_final = has_items_tw(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/march_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79ce0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 41.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 31000 32000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 43.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 32000 33000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 53.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 33000 34000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:19<00:00, 51.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 34000 35000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:21<00:00, 47.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 35000 36000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 40.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 36000 37000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:15<00:00, 65.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37000 38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 632/632 [00:09<00:00, 64.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 38000 39000\n"
     ]
    }
   ],
   "source": [
    "for i in range(31, 39):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 1000*i \n",
    "    b = 1000*(i+1)\n",
    "    time.sleep(5)\n",
    "    for e in tqdm(onlyfiles[a:b]):\n",
    "        df = pd.read_parquet(f'{path_tw}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final[df_vars]\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final = compute_engagement_tw(df_final)\n",
    "    df_final = has_items_tw(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/april_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6ccbb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29688"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tw, base, rand, baseline, agg, agg_base = get_path('SA', 'april2')\n",
    "onlyfiles = [f for f in listdir(path_tw) if isfile(join(path_tw, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2095c1f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:47<00:00, 62.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:59<00:00, 50.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3000 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [01:04<00:00, 46.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6000 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:55<00:00, 53.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 9000 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:53<00:00, 56.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 12000 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:51<00:00, 57.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 15000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:57<00:00, 51.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 18000 21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:57<00:00, 52.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 21000 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:57<00:00, 51.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 24000 27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2688/2688 [00:43<00:00, 62.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 27000 30000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in tqdm(onlyfiles[a:b]):\n",
    "        df = pd.read_parquet(f'{path_tw}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/april2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf2fabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29010"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tw, base, rand, baseline, agg, agg_base = get_path('SA', 'post_treat2')\n",
    "onlyfiles = [f for f in listdir(path_tw) if isfile(join(path_tw, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "380193a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 21000 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [53:08<1:46:16, 3188.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 24000 27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [1:22:47<39:19, 2359.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 27000 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:41:25<00:00, 2028.51s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(7, 10)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{path_tw}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang2'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/posttreat2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3d9afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29560"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_tw, base, rand, baseline, agg, agg_base = get_path('SA', 'april')\n",
    "onlyfiles = [f for f in listdir(path_tw) if isfile(join(path_tw, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e6c7e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [42:25<6:21:46, 2545.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 3000 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [1:13:18<4:45:05, 2138.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 6000 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [1:45:20<3:57:57, 2039.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 9000 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [2:10:09<3:02:12, 1822.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 12000 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [2:31:15<2:15:07, 1621.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 15000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [2:53:38<1:41:48, 1527.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 18000 21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [3:17:21<1:14:39, 1493.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 21000 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [3:38:48<47:34, 1427.24s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 24000 27000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [4:02:21<23:42, 1422.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 27000 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [4:21:22<00:00, 1568.28s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0, 10)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{path_tw}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang2'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/april1_good{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42f1604d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19178"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, agg, agg_base = get_path('KE', 'april')\n",
    "path_tw1 = f'../../data/03-experiment/{country}/treatment/followers/00-raw/tweets_batch2/may/'\n",
    "onlyfiles = [f for f in listdir(path_tw1) if isfile(join(path_tw1, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc55050d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 15000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [35:59<35:59, 2159.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 18000 21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [52:17<00:00, 1568.71s/it]\n"
     ]
    }
   ],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, agg, agg_base = get_path(country, 'april')\n",
    "path_tw1 = f'../../data/03-experiment/{country}/treatment/followers/00-raw/tweets_batch2/may/'\n",
    "onlyfiles = [f for f in listdir(path_tw1) if isfile(join(path_tw1, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "\n",
    "for i in tqdm(range(5, 7)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{path_tw1}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang2'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/may_batch2{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44c46050",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19178"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, agg, agg_base = get_path(country, 'april')\n",
    "path_tw1 = f'../../data/03-experiment/{country}/treatment/followers/00-raw/tweets_batch2/may/'\n",
    "onlyfiles = [f for f in listdir(path_tw1) if isfile(join(path_tw1, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "76d7c06053c3456e5600312cec90888656fc0ed30c03d8425b9dac6e4fc8e014"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
