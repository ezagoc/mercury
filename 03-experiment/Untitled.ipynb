{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1f5211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve Content Shared By Influencers During Treatment\n",
    "\n",
    "-- keywords: #FactsMatter, @AfricaCheck\n",
    "-- start date: 13-03-23\n",
    "-- end date:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from tweetple import TweetPle\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from funcs import *\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from general import *\n",
    "from scrape import *\n",
    "\n",
    "\n",
    "def start_date(days_ago):\n",
    "    \"\"\"Start date of scrape\"\"\"\n",
    "    start = str(date.today() - timedelta(days_ago))\n",
    "    start = start + \"T00:00:00Z\"\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def end_date(days_ago):\n",
    "    \"\"\"Start date of scrape\"\"\"\n",
    "    end = str(date.today() - timedelta(days_ago))\n",
    "    end = end + \"T23:59:59Z\"\n",
    "\n",
    "    return end\n",
    "\n",
    "\n",
    "def scrape_tweets(accounts, path, bearer_token, days_ago):\n",
    "    \"\"\"Scrape Tweets\"\"\"\n",
    "    end = end_date(days_ago - 6)\n",
    "    TweetPle.TweetStreamer(\n",
    "        accounts, bearer_token, path, start_date(days_ago), end\n",
    "    ).main()\n",
    "\n",
    "    return print(\"Content scraped\")\n",
    "\n",
    "\n",
    "def get_paths(days_ago, country):\n",
    "\n",
    "    base = f\"../../data/03-experiment/{country}/treatment/influencers/\"\n",
    "    rest = timedelta(days_ago - 6)\n",
    "    datep = str(date.today() - rest)\n",
    "    path_tw = base + \"00-raw/twitter_b2/\" + datep\n",
    "\n",
    "    return path_tw, base, datep\n",
    "\n",
    "\n",
    "def tweet_types(path_read):\n",
    "    df = read_files(path_read)\n",
    "    df.referenced_tweets = df.referenced_tweets.fillna({i: {} for i in df.index})\n",
    "    m = pd.DataFrame(df[\"referenced_tweets\"].tolist())\n",
    "    m = m[0].apply(pd.Series)\n",
    "    m.replace(\n",
    "        {\n",
    "            \"retweeted\": \"retweet\",\n",
    "            \"replied_to\": \"reply\",\n",
    "            \"quoted\": \"quote\",\n",
    "            np.nan: \"tweet\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df[\"type\"] = m[\"type\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(\"referenced_tweets\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def content_twitter(df):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    pattern = '|'.join([f'(?i){word}' for word in words_tweets])\n",
    "    pattern_url = '|'.join([f'(?i){url}' for url in urls_list])\n",
    "    # df = tweet_types(path)\n",
    "    df = df\n",
    "    df = expand_column(df, \"entities.hashtags\")\n",
    "    df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    df = expand_column(df, \"entities.urls\")\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    df0['campaign_hashtag'] = 1\n",
    "    df1 = df.loc[df[\"text\"].str.contains(\"@africacheck\", case=False)]\n",
    "    df1['campaign_hashtag'] = 1\n",
    "    df2 = df.loc[df[\"text\"].str.contains(pattern, case=False)]\n",
    "    df2 = df2[~df2['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    df2['campaign_hashtag'] = 0\n",
    "    df3 = df.loc[df[\"expanded_url\"].str.contains(pattern_url, case=False, na=False)]\n",
    "    df3 = df3[~df3['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    df3['campaign_hashtag'] = 0\n",
    "    df = df0.append(df1).append(df2).append(df3).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    try:\n",
    "        df[\"retweet\"] = np.where(df.url.isna(), 1, 0)\n",
    "    except:\n",
    "        df[\"url\"] = np.nan\n",
    "        df[\"retweet\"] = 1\n",
    "    df = df[df[\"retweet\"] != 1]\n",
    "    df = df.drop(\"retweet\", axis=1)\n",
    "    df[\"tweet_url\"] = \"https://twitter.com/\" + df.handle + \"/status/\" + df.id\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "def content_twitter_old(path):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    # df = tweet_types(path)\n",
    "    df = read_files(path)\n",
    "    df = expand_column(df, \"entities.hashtags\")\n",
    "    df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    df1 = df.loc[df[\"text\"].str.contains(\"@africacheck\", case=False)]\n",
    "    df = df0.append(df1).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    df = expand_column(df, \"entities.urls\")\n",
    "    try:\n",
    "        df[\"retweet\"] = np.where(df.url.isna(), 1, 0)\n",
    "    except:\n",
    "        df[\"url\"] = np.nan\n",
    "        df[\"retweet\"] = 1\n",
    "    df = df[df[\"retweet\"] != 1]\n",
    "    df = df.drop(\"retweet\", axis=1)\n",
    "    df[\"tweet_url\"] = \"https://twitter.com/\" + df.handle + \"/status/\" + df.id\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def incremental_range(start, stop, step, inc):\n",
    "    value = start\n",
    "    while value < stop:\n",
    "        yield value\n",
    "        value += step\n",
    "        step += inc\n",
    "\n",
    "\n",
    "def create_list(df, start, step, inc):\n",
    "    stop = len(df.columns)\n",
    "    a_list = list(incremental_range(start, stop, step, inc))\n",
    "\n",
    "    return a_list\n",
    "\n",
    "\n",
    "def create_dictionaries(contents, n_contents, a_list, name1, name2):\n",
    "    a_dict = dict(zip(contents, a_list))\n",
    "    b_dict = dict(zip(n_contents, a_list))\n",
    "    a_dict = {k: name1 + v for k, v in a_dict.items()}\n",
    "    b_dict = {k: name2 + v for k, v in b_dict.items()}\n",
    "\n",
    "    return a_dict, b_dict\n",
    "\n",
    "\n",
    "def rename_columns(df, a_dict, b_dict):\n",
    "    for key, value in a_dict.items():\n",
    "        df.columns.values[key] = value\n",
    "    for key, value in b_dict.items():\n",
    "        df.columns.values[key] = value\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_duplicated_columns(df):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def problematic(df, t_col, n_col, str_find):\n",
    "    prob = [col for col in df.columns if str_find in col]\n",
    "    df[\"auxiliar\"] = df[prob].sum(axis=1)\n",
    "    df[n_col] = np.where((df[\"auxiliar\"] == 0) & (df[t_col] == 1), 1, 0)\n",
    "    df = df.drop([\"auxiliar\"], axis=1)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1e59a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "csv_folder = Path('../../data/03-experiment/SA/treatment/influencers/00-raw/twitter_b2/')  # path to your folder, e.g. to `2022`\n",
    "df = pd.concat(pd.read_parquet(p) for p in csv_folder.glob('**/*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8215922",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def africa_report(path_read, datep):\n",
    "    \"\"\"Generate report for AfricaCheck\"\"\"\n",
    "    base = path_read.split(\"/*/\")[0]\n",
    "    df = pd.concat(map(pd.read_excel, glob.glob(f\"{path_read}/twitter.xlsx\")), axis=1)\n",
    "    contents = create_list(df, 5, 7, 0)\n",
    "    n_contents = create_list(df, 6, 7, 0)\n",
    "    stop = len(df.columns)\n",
    "    weeks = list(map(str, range(1, int(stop / 7) + 1)))\n",
    "    n_contents_dict, contents_dict = create_dictionaries(\n",
    "        contents, n_contents, weeks, \"content_w\", \"n_content_w\"\n",
    "    )\n",
    "    df = rename_columns(df, contents_dict, n_contents_dict)\n",
    "    df = drop_duplicated_columns(df)\n",
    "    df = problematic(df, \"treatment\", \"potentially_problematic\", \"content_w\")\n",
    "    df.to_excel(f\"{base}/{datep}/tracker_twitter.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# def monitor_influencers(days_ago=1):\n",
    "# --38, 31, 24, 17, 10\n",
    "def monitor_influencers(df, days_ago=17, country='SA'):\n",
    "    \"\"\"Run process\"\"\"\n",
    "    # Twitter\n",
    "    _, _, _, _, bearer_token, _,_ = twitter_credentials(\n",
    "        \"../../conf/credentials.yaml\"\n",
    "    )\n",
    "    participants_tw = get_participants_twitter(country)\n",
    "    usernames_tw = list(participants_tw[\"username\"])\n",
    "    path_tw, base, datep = get_paths(days_ago, country)\n",
    "    #create_folder(path_tw)\n",
    "    #scrape_tweets(usernames_tw, f\"{path_tw}/\", bearer_token, days_ago)\n",
    "    df_tw = content_twitter(df)\n",
    "    create_folder(f\"{base}01-preprocessed/batch2/Agg\")\n",
    "    df_tw.to_excel(f\"{base}01-preprocessed/batch2/Agg/tweets_agg.xlsx\", index=False)\n",
    "    count_tw = df_tw.groupby(\"handle\").count()\n",
    "    count_tw_2 = df_tw.groupby(\"handle\").sum()\n",
    "    count_tw = count_tw.reset_index()[[\"handle\", \"id\"]]\n",
    "    count_tw_2 = count_tw_2.reset_index()[[\"handle\",'public_metrics.impression_count',\n",
    "                                       'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                                       'public_metrics.reply_count', 'public_metrics.retweet_count']]\n",
    "    count_tw = count_tw.rename({\"id\": \"n_content\", \"handle\": \"username\",\n",
    "                                'public_metrics.impression_count': \"n_impressions\",\n",
    "                                'public_metrics.like_count':'n_likes', 'public_metrics.quote_count':'n_quotes',\n",
    "                                'public_metrics.reply_count':'n_replies', 'public_metrics.retweet_count':'n_retweets'}, axis=1)\n",
    "    count_tw_2 = count_tw_2.rename({\"handle\": \"username\",\n",
    "                                'public_metrics.impression_count': \"n_impressions\",\n",
    "                                'public_metrics.like_count':'n_likes', 'public_metrics.quote_count':'n_quotes',\n",
    "                                'public_metrics.reply_count':'n_replies', 'public_metrics.retweet_count':'n_retweets'}, axis=1)\n",
    "    count_tw = count_tw.merge(count_tw_2, how=\"left\", on=\"username\")\n",
    "    found = list(df_tw.handle.unique())\n",
    "    participants_tw[\"content\"] = np.where(participants_tw[\"username\"].isin(found), 1, 0)\n",
    "    create_folder(f\"{base}01-preprocessed/batch2/report/Agg\")\n",
    "    participants_tw = participants_tw.merge(count_tw, how=\"left\", on=\"username\")\n",
    "    participants_tw[[\"n_content\",\"n_impressions\", \n",
    "                 \"n_likes\",\"n_quotes\", \"n_replies\", \n",
    "                 \"n_retweets\"]] = participants_tw[[\"n_content\",\"n_impressions\", \n",
    "                                                  \"n_likes\",\"n_quotes\",\n",
    "                                                  \"n_replies\", \"n_retweets\"]].fillna(0)\n",
    "    participants_tw.to_excel(\n",
    "        f\"{base}01-preprocessed/batch2/report/Agg/twitter.xlsx\", index=False\n",
    "    )\n",
    "    summ_tw = participants_tw.groupby([\"treatment\"]).sum().reset_index()\n",
    "    summ_tw = summ_tw[[\"treatment\", \"content\"]]\n",
    "    summ_tw = summ_tw.rename({\"content\": \"percent_share\"}, axis=1)\n",
    "    summ_tw[\"percent_share\"] = (summ_tw[\"percent_share\"] / 26) * 100\n",
    "    summ_tw.to_excel(\n",
    "        f\"{base}01-preprocessed/batch2/report/Agg/summary_twitter.xlsx\", index=False\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "270f723f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ec834a3e4cf5>:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df0['campaign_hashtag'] = 1\n",
      "<ipython-input-2-ec834a3e4cf5>:94: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['campaign_hashtag'] = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../../data/03-experiment/SA/treatment/influencers/01-preprocessed/batch2/Agg  already exists\n",
      "Directory  ../../data/03-experiment/SA/treatment/influencers/01-preprocessed/batch2/report/Agg  created \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ec834a3e4cf5>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df0.append(df1).append(df2).append(df3).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
      "<ipython-input-2-ec834a3e4cf5>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df0.append(df1).append(df2).append(df3).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
      "<ipython-input-2-ec834a3e4cf5>:101: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df0.append(df1).append(df2).append(df3).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
      "<ipython-input-20-fd69084c8b7e>:35: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  count_tw_2 = df_tw.groupby(\"handle\").sum()\n",
      "<ipython-input-20-fd69084c8b7e>:61: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  summ_tw = participants_tw.groupby([\"treatment\"]).sum().reset_index()\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    monitor_influencers(df, 1, \"SA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
