{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4ea5b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from tweetple import TweetPle\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from funcs import *\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from general import *\n",
    "from scrape import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9edf23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Retrieve Content Shared By Influencers During Treatment\n",
    "\n",
    "-- keywords: #FactsMatter, @AfricaCheck\n",
    "-- start date: 13-03-23\n",
    "-- end date:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "from tweetple import TweetPle\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from funcs import *\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from general import *\n",
    "from scrape import *\n",
    "\n",
    "\n",
    "def start_date(days_ago):\n",
    "    \"\"\"Start date of scrape\"\"\"\n",
    "    start = str(date.today() - timedelta(days_ago))\n",
    "    start = start + \"T00:00:00Z\"\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def end_date(days_ago):\n",
    "    \"\"\"Start date of scrape\"\"\"\n",
    "    end = str(date.today() - timedelta(days_ago))\n",
    "    end = end + \"T23:59:59Z\"\n",
    "\n",
    "    return end\n",
    "\n",
    "\n",
    "def scrape_tweets(accounts, path, bearer_token, days_ago):\n",
    "    \"\"\"Scrape Tweets\"\"\"\n",
    "    end = end_date(days_ago - 6)\n",
    "    TweetPle.TweetStreamer(\n",
    "        accounts, bearer_token, path, start_date(days_ago), end\n",
    "    ).main()\n",
    "\n",
    "    return print(\"Content scraped\")\n",
    "\n",
    "\n",
    "def get_paths(days_ago, country):\n",
    "\n",
    "    base = f\"../../data/03-experiment/{country}/treatment/influencers/\"\n",
    "    rest = timedelta(days_ago - 6)\n",
    "    datep = str(date.today() - rest)\n",
    "    path_tw = base + \"00-raw/twitter/\" + datep\n",
    "\n",
    "    return path_tw, base, datep\n",
    "\n",
    "\n",
    "def tweet_types(path_read):\n",
    "    df = read_files(path_read)\n",
    "    df.referenced_tweets = df.referenced_tweets.fillna({i: {} for i in df.index})\n",
    "    m = pd.DataFrame(df[\"referenced_tweets\"].tolist())\n",
    "    m = m[0].apply(pd.Series)\n",
    "    m.replace(\n",
    "        {\n",
    "            \"retweeted\": \"retweet\",\n",
    "            \"replied_to\": \"reply\",\n",
    "            \"quoted\": \"quote\",\n",
    "            np.nan: \"tweet\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df[\"type\"] = m[\"type\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.drop(\"referenced_tweets\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def content_twitter(path):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    pattern = '|'.join([f'(?i){word}' for word in words_tweets])\n",
    "    pattern_url = '|'.join([f'(?i){url}' for url in urls_list])\n",
    "    # df = tweet_types(path)\n",
    "    df = read_files(path)\n",
    "    df = expand_column(df, \"entities.hashtags\")\n",
    "    df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    df = expand_column(df, \"entities.urls\")\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    df0['campaign_hashtag'] = 1\n",
    "    df1 = df.loc[df[\"text\"].str.contains(\"@africacheck\", case=False)]\n",
    "    df1['campaign_hashtag'] = 1\n",
    "    df2 = df.loc[df[\"text\"].str.contains(pattern, case=False)]\n",
    "    df2 = df2[~df2['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    df2['campaign_hashtag'] = 0\n",
    "    df3 = df.loc[df[\"expanded_url\"].str.contains(pattern_url, case=False, na=False)]\n",
    "    df3 = df3[~df3['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    df3['campaign_hashtag'] = 0\n",
    "    df = df0.append(df1).append(df2).append(df3).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    try:\n",
    "        df[\"retweet\"] = np.where(df.url.isna(), 1, 0)\n",
    "    except:\n",
    "        df[\"url\"] = np.nan\n",
    "        df[\"retweet\"] = 1\n",
    "    df = df[df[\"retweet\"] != 1]\n",
    "    df = df.drop(\"retweet\", axis=1)\n",
    "    df[\"tweet_url\"] = \"https://twitter.com/\" + df.handle + \"/status/\" + df.id\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n",
    "\n",
    "def content_twitter_old(path):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    # df = tweet_types(path)\n",
    "    df = read_files(path)\n",
    "    df = expand_column(df, \"entities.hashtags\")\n",
    "    df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    df1 = df.loc[df[\"text\"].str.contains(\"@africacheck\", case=False)]\n",
    "    df = df0.append(df1).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    df = expand_column(df, \"entities.urls\")\n",
    "    try:\n",
    "        df[\"retweet\"] = np.where(df.url.isna(), 1, 0)\n",
    "    except:\n",
    "        df[\"url\"] = np.nan\n",
    "        df[\"retweet\"] = 1\n",
    "    df = df[df[\"retweet\"] != 1]\n",
    "    df = df.drop(\"retweet\", axis=1)\n",
    "    df[\"tweet_url\"] = \"https://twitter.com/\" + df.handle + \"/status/\" + df.id\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def incremental_range(start, stop, step, inc):\n",
    "    value = start\n",
    "    while value < stop:\n",
    "        yield value\n",
    "        value += step\n",
    "        step += inc\n",
    "\n",
    "\n",
    "def create_list(df, start, step, inc):\n",
    "    stop = len(df.columns)\n",
    "    a_list = list(incremental_range(start, stop, step, inc))\n",
    "\n",
    "    return a_list\n",
    "\n",
    "\n",
    "def create_dictionaries(contents, n_contents, a_list, name1, name2):\n",
    "    a_dict = dict(zip(contents, a_list))\n",
    "    b_dict = dict(zip(n_contents, a_list))\n",
    "    a_dict = {k: name1 + v for k, v in a_dict.items()}\n",
    "    b_dict = {k: name2 + v for k, v in b_dict.items()}\n",
    "\n",
    "    return a_dict, b_dict\n",
    "\n",
    "\n",
    "def rename_columns(df, a_dict, b_dict):\n",
    "    for key, value in a_dict.items():\n",
    "        df.columns.values[key] = value\n",
    "    for key, value in b_dict.items():\n",
    "        df.columns.values[key] = value\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_duplicated_columns(df):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def problematic(df, t_col, n_col, str_find):\n",
    "    prob = [col for col in df.columns if str_find in col]\n",
    "    df[\"auxiliar\"] = df[prob].sum(axis=1)\n",
    "    df[n_col] = np.where((df[\"auxiliar\"] == 0) & (df[t_col] == 1), 1, 0)\n",
    "    df = df.drop([\"auxiliar\"], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87cd2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet(\"/Users/joaquinbarrutia/Dropbox/Bolivia_Project/social-media-influencers-af/data/03-experiment/KE/treatment/influencers/00-raw/twitter/2023-05-07/2314837515.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db1bfbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '|'.join([f'(?i){word}' for word in words_tweets])\n",
    "pattern_url = '|'.join([f'(?i){url}' for url in urls_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "64fc7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_twitter(path):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    pattern = '|'.join([f'(?i){word}' for word in words_tweets])\n",
    "    pattern_url = '|'.join([f'(?i){url}' for url in urls_list])\n",
    "    # df = tweet_types(path)\n",
    "    df = read_files(path)\n",
    "    df = df[~df['text'].isnull()]\n",
    "    #df = expand_column(df, \"entities.hashtags\")\n",
    "    #df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    #df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    #df = expand_column(df, \"entities.urls\")\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    df0['campaign_hashtag'] = 1\n",
    "    df1 = df.loc[df[\"text\"].str.contains(\"@africacheck\", case=False)]\n",
    "    df1['campaign_hashtag'] = 1\n",
    "    df2 = df.loc[df[\"text\"].str.contains(pattern, case=False)]\n",
    "    df2 = df2[~df2['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    df2['campaign_hashtag'] = 0\n",
    "    #df3 = df.loc[df[\"expanded_url\"].str.contains(pattern_url, case=False, na=False)]\n",
    "    #df3 = df3[~df3['id'].isin(list(list(df0.id)+list(df1.id)))]\n",
    "    #df3['campaign_hashtag'] = 0\n",
    "    df = df0.append(df1).append(df2).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
    "    df = df[df['type']!='retweeted']\n",
    "    #try:\n",
    "     #   df[\"retweet\"] = np.where(df.url.isna(), 1, 0)\n",
    "    #except:\n",
    "     #   df[\"url\"] = np.nan\n",
    "      #  df[\"retweet\"] = 1\n",
    "    #df = df[df[\"retweet\"] != 1]\n",
    "    #df = df.drop(\"retweet\", axis=1)\n",
    "    #df[\"tweet_url\"] = \"https://twitter.com/\" + df.handle + \"/status/\" + df.id\n",
    "    df.reset_index(drop=True,inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "343d0bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_twitter(path):\n",
    "    \"\"\"Find Tweets containing #FactsMatter\"\"\"\n",
    "    pattern = '|'.join([f'(?i){word}' for word in words_tweets])\n",
    "    pattern_url = '|'.join([f'(?i){url}' for url in urls_list])\n",
    "    # df = tweet_types(path)\n",
    "    df = read_files(path)\n",
    "    df = df[~df['text'].isnull()]\n",
    "    #df = expand_column(df, \"entities.hashtags\")\n",
    "    #df.rename({\"tag\": \"hashtag\"}, axis=1, inplace=1)\n",
    "    #df = df.drop([\"end\", \"start\"], axis=1)\n",
    "    #df = expand_column(df, \"entities.urls\")\n",
    "    df0 = df.loc[df[\"text\"].str.contains(\"#factsmatter\", case=False)]\n",
    "    return df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4fe11ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-75321309e21a>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df0['campaign_hashtag'] = 1\n",
      "<ipython-input-54-75321309e21a>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['campaign_hashtag'] = 1\n",
      "<ipython-input-54-75321309e21a>:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df0.append(df1).append(df2).drop_duplicates(subset=\"id\").reset_index(drop=True)\n",
      "<ipython-input-54-75321309e21a>:22: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df0.append(df1).append(df2).drop_duplicates(subset=\"id\").reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_tw = content_twitter(f\"/Users/joaquinbarrutia/Dropbox/Bolivia_Project/social-media-influencers-af/data/03-experiment/KE/treatment/influencers/00-raw/twitter/2023-05-07/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a12d70ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>impression_count</th>\n",
       "      <th>campaign_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343259811</td>\n",
       "      <td>1653300807779975171</td>\n",
       "      <td>1653299165550505986</td>\n",
       "      <td>2023-05-02T07:30:04.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>#FactsMatter Don't be scammed. @AfricaCheck ht...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3257710179</td>\n",
       "      <td>1653379400447021059</td>\n",
       "      <td>1653370251516542976</td>\n",
       "      <td>2023-05-02T12:42:22.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@odhiambo_10 @AfricaCheck Scammers have no pla...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    author_id                   id      conversation_id  \\\n",
       "0   343259811  1653300807779975171  1653299165550505986   \n",
       "1  3257710179  1653379400447021059  1653370251516542976   \n",
       "\n",
       "                 created_at        type  \\\n",
       "0  2023-05-02T07:30:04.000Z  replied_to   \n",
       "1  2023-05-02T12:42:22.000Z  replied_to   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0  #FactsMatter Don't be scammed. @AfricaCheck ht...            0.0   \n",
       "1  @odhiambo_10 @AfricaCheck Scammers have no pla...            0.0   \n",
       "\n",
       "   reply_count  like_count  quote_count  impression_count  campaign_hashtag  \n",
       "0          0.0         0.0          0.0              27.0                 1  \n",
       "1          0.0         0.0          0.0               4.0                 1  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d4ad2bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start =  '2023-05-01T00:00:00Z'\n",
    "end = '2023-05-07T00:00:00Z'\n",
    "bearer = \"AAAAAAAAAAAAAAAAAAAAAFpgZAEAAAAAbJS59UWzipi32ixd7LHtXov9olo%3D7gxD8Afshgj4munMXHLU08jzRdTpsAh4RZqq7VBofq1wAvkx1T\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10283a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52b1c7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mTweetPle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTweetStreamer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1126820633244065792\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbearer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/joaquinbarrutia/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tweetple/TweetPle.py:262\u001b[0m, in \u001b[0;36mTweetStreamer.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m input_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_str\u001b[38;5;241m.\u001b[39misnumeric():\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# list of tweet ids\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstreamer_tweetids\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m validators\u001b[38;5;241m.\u001b[39murl(input_str) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# list of urls\u001b[39;00m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreamer_links()\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tweetple/TweetPle.py:226\u001b[0m, in \u001b[0;36mTweetStreamer.streamer_tweetids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prev, curr \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(bounds, bounds[\u001b[38;5;241m1\u001b[39m:])):\n\u001b[1;32m    224\u001b[0m     stat \u001b[38;5;241m=\u001b[39m GetStatsFromTweets(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[prev:curr], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbearer_token)\n\u001b[1;32m    225\u001b[0m     df_stats \u001b[38;5;241m=\u001b[39m df_stats\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 226\u001b[0m         \u001b[43mstat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    227\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m df_stats\u001b[38;5;241m.\u001b[39mto_parquet(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_save\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    230\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mstr\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)))\n",
      "File \u001b[0;32m~/miniforge3/envs/env_tf/lib/python3.9/site-packages/tweetple/TwitterFullArchive.py:529\u001b[0m, in \u001b[0;36mGetStatsFromTweets.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    527\u001b[0m headers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_headers(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbearer_token)\n\u001b[1;32m    528\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect_to_endpoint(headers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtweets_ids)\n\u001b[0;32m--> 529\u001b[0m data \u001b[38;5;241m=\u001b[39m json_normalize(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    530\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_consulted\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(date\u001b[38;5;241m.\u001b[39mtoday())\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "TweetPle.TweetStreamer('1126820633244065792', bearer, '/Users/joaquinbarrutia/',start, end ).main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8990b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f89caff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>impression_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>503359879</td>\n",
       "      <td>1655129258350575616</td>\n",
       "      <td>1655127830311899136</td>\n",
       "      <td>2023-05-07T08:35:40.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@TheMiddleBorn @wwwasike</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>246769503</td>\n",
       "      <td>1655255265510318082</td>\n",
       "      <td>1655163955956920320</td>\n",
       "      <td>2023-05-07T16:56:23.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@DM_holdings Has nothing to do with the econom...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16435650</td>\n",
       "      <td>1654186791040106498</td>\n",
       "      <td>1654157750560079873</td>\n",
       "      <td>2023-05-04T18:10:38.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@imbuyuz The proposed increase looks like one ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270834723</td>\n",
       "      <td>1652995999634161666</td>\n",
       "      <td>1652995999634161666</td>\n",
       "      <td>2023-05-01T11:18:52.000Z</td>\n",
       "      <td>None</td>\n",
       "      <td>We celebrate you #TeamPR. Happy Labour Day. ht...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58806221</td>\n",
       "      <td>1654728520164057089</td>\n",
       "      <td>1654728520164057089</td>\n",
       "      <td>2023-05-06T06:03:17.000Z</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>RT @SemaUkweliKenya: David Sadera Munyakei: Po...</td>\n",
       "      <td>435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4895104294</td>\n",
       "      <td>1653637345764159489</td>\n",
       "      <td>1653637345764159489</td>\n",
       "      <td>2023-05-03T05:47:20.000Z</td>\n",
       "      <td>retweeted</td>\n",
       "      <td>RT @UNCCD: Meet our Goodwill Ambassador @baaba...</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17700414</td>\n",
       "      <td>1654435834894905346</td>\n",
       "      <td>1654434054186442752</td>\n",
       "      <td>2023-05-05T10:40:15.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@deeRsquared Amepeleka detectives wa Assmio</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1291061156</td>\n",
       "      <td>1655309552311476226</td>\n",
       "      <td>1655309145870876672</td>\n",
       "      <td>2023-05-07T20:32:06.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>Tiwa Savage stunning at the #CoronationConcert...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1055.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2497334080</td>\n",
       "      <td>1655266931375501312</td>\n",
       "      <td>1655265090856267779</td>\n",
       "      <td>2023-05-07T17:42:44.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@berylsavanah We try again next Sunday</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2314837515</td>\n",
       "      <td>1655051017921212418</td>\n",
       "      <td>1655036014719557632</td>\n",
       "      <td>2023-05-07T03:24:46.000Z</td>\n",
       "      <td>replied_to</td>\n",
       "      <td>@RadioCitizenFM Amongura Teso nabarikiwa #Pamb...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     author_id                   id      conversation_id  \\\n",
       "0    503359879  1655129258350575616  1655127830311899136   \n",
       "0    246769503  1655255265510318082  1655163955956920320   \n",
       "0     16435650  1654186791040106498  1654157750560079873   \n",
       "0    270834723  1652995999634161666  1652995999634161666   \n",
       "0     58806221  1654728520164057089  1654728520164057089   \n",
       "..         ...                  ...                  ...   \n",
       "0   4895104294  1653637345764159489  1653637345764159489   \n",
       "0     17700414  1654435834894905346  1654434054186442752   \n",
       "0   1291061156  1655309552311476226  1655309145870876672   \n",
       "0   2497334080  1655266931375501312  1655265090856267779   \n",
       "0   2314837515  1655051017921212418  1655036014719557632   \n",
       "\n",
       "                  created_at        type  \\\n",
       "0   2023-05-07T08:35:40.000Z  replied_to   \n",
       "0   2023-05-07T16:56:23.000Z  replied_to   \n",
       "0   2023-05-04T18:10:38.000Z  replied_to   \n",
       "0   2023-05-01T11:18:52.000Z        None   \n",
       "0   2023-05-06T06:03:17.000Z   retweeted   \n",
       "..                       ...         ...   \n",
       "0   2023-05-03T05:47:20.000Z   retweeted   \n",
       "0   2023-05-05T10:40:15.000Z  replied_to   \n",
       "0   2023-05-07T20:32:06.000Z  replied_to   \n",
       "0   2023-05-07T17:42:44.000Z  replied_to   \n",
       "0   2023-05-07T03:24:46.000Z  replied_to   \n",
       "\n",
       "                                                 text  retweet_count  \\\n",
       "0                            @TheMiddleBorn @wwwasike            0.0   \n",
       "0   @DM_holdings Has nothing to do with the econom...            0.0   \n",
       "0   @imbuyuz The proposed increase looks like one ...            0.0   \n",
       "0   We celebrate you #TeamPR. Happy Labour Day. ht...            5.0   \n",
       "0   RT @SemaUkweliKenya: David Sadera Munyakei: Po...          435.0   \n",
       "..                                                ...            ...   \n",
       "0   RT @UNCCD: Meet our Goodwill Ambassador @baaba...           21.0   \n",
       "0         @deeRsquared Amepeleka detectives wa Assmio            0.0   \n",
       "0   Tiwa Savage stunning at the #CoronationConcert...            0.0   \n",
       "0              @berylsavanah We try again next Sunday            0.0   \n",
       "0   @RadioCitizenFM Amongura Teso nabarikiwa #Pamb...            0.0   \n",
       "\n",
       "    reply_count  like_count  quote_count  impression_count  \n",
       "0           1.0        22.0          0.0              45.0  \n",
       "0           0.0         3.0          0.0             245.0  \n",
       "0           0.0         1.0          0.0              13.0  \n",
       "0           0.0        20.0          0.0            1109.0  \n",
       "0           0.0         0.0          0.0               0.0  \n",
       "..          ...         ...          ...               ...  \n",
       "0           0.0         0.0          0.0               0.0  \n",
       "0           0.0         0.0          0.0              27.0  \n",
       "0           0.0         5.0          0.0            1055.0  \n",
       "0           0.0         1.0          0.0               6.0  \n",
       "0           0.0         0.0          0.0               1.0  \n",
       "\n",
       "[70 rows x 11 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates('author_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2be0bca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>impression_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author_id, id, conversation_id, created_at, type, text, retweet_count, reply_count, like_count, quote_count, impression_count]\n",
       "Index: []"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['author_id']==1126820633244065792]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1386f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def africa_report(path_read, datep):\n",
    "    \"\"\"Generate report for AfricaCheck\"\"\"\n",
    "    base = path_read.split(\"/*/\")[0]\n",
    "    df = pd.concat(map(pd.read_excel, glob.glob(f\"{path_read}/twitter.xlsx\")), axis=1)\n",
    "    contents = create_list(df, 5, 7, 0)\n",
    "    n_contents = create_list(df, 6, 7, 0)\n",
    "    stop = len(df.columns)\n",
    "    weeks = list(map(str, range(1, int(stop / 7) + 1)))\n",
    "    n_contents_dict, contents_dict = create_dictionaries(\n",
    "        contents, n_contents, weeks, \"content_w\", \"n_content_w\"\n",
    "    )\n",
    "    df = rename_columns(df, contents_dict, n_contents_dict)\n",
    "    df = drop_duplicated_columns(df)\n",
    "    df = problematic(df, \"treatment\", \"potentially_problematic\", \"content_w\")\n",
    "    df.to_excel(f\"{base}/{datep}/tracker_twitter.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# def monitor_influencers(days_ago=1):\n",
    "# --35, 28, 21, 14\n",
    "def monitor_influencers(days_ago=7, country='KE'):\n",
    "    \"\"\"Run process\"\"\"\n",
    "    # Twitter\n",
    "    _, _, _, bearer_token, _, _,_ = twitter_credentials(\n",
    "        \"../../conf/credentials.yaml\"\n",
    "    )\n",
    "    participants_tw = get_participants_twitter(country)\n",
    "    usernames_tw = list(participants_tw[\"username\"])\n",
    "    path_tw, base, datep = get_paths(days_ago, country)\n",
    "    create_folder(path_tw)\n",
    "    scrape_tweets(usernames_tw, f\"{path_tw}/\", bearer_token, days_ago)\n",
    "    df_tw = content_twitter(f\"{path_tw}\")\n",
    "    create_folder(f\"{base}01-preprocessed/content/{datep}\")\n",
    "    df_tw.to_excel(f\"{base}01-preprocessed/content/{datep}/tweets.xlsx\", index=False)\n",
    "    count_tw = df_tw.groupby(\"handle\").count()\n",
    "    count_tw_2 = df_tw.groupby(\"handle\").sum()\n",
    "    count_tw = count_tw.reset_index()[[\"handle\", \"id\"]]\n",
    "    count_tw_2 = count_tw_2.reset_index()[[\"handle\",'public_metrics.impression_count',\n",
    "                                       'public_metrics.like_count', 'public_metrics.quote_count',\n",
    "                                       'public_metrics.reply_count', 'public_metrics.retweet_count']]\n",
    "    count_tw = count_tw.rename({\"id\": \"n_content\", \"handle\": \"username\",\n",
    "                                'public_metrics.impression_count': \"n_impressions\",\n",
    "                                'public_metrics.like_count':'n_likes', 'public_metrics.quote_count':'n_quotes',\n",
    "                                'public_metrics.reply_count':'n_replies', 'public_metrics.retweet_count':'n_retweets'}, axis=1)\n",
    "    count_tw_2 = count_tw_2.rename({\"handle\": \"username\",\n",
    "                                'public_metrics.impression_count': \"n_impressions\",\n",
    "                                'public_metrics.like_count':'n_likes', 'public_metrics.quote_count':'n_quotes',\n",
    "                                'public_metrics.reply_count':'n_replies', 'public_metrics.retweet_count':'n_retweets'}, axis=1)\n",
    "    count_tw = count_tw.merge(count_tw_2, how=\"left\", on=\"username\")\n",
    "    found = list(df_tw.handle.unique())\n",
    "    participants_tw[\"content\"] = np.where(participants_tw[\"username\"].isin(found), 1, 0)\n",
    "    create_folder(f\"{base}01-preprocessed/report/{datep}\")\n",
    "    participants_tw = participants_tw.merge(count_tw, how=\"left\", on=\"username\")\n",
    "    participants_tw[[\"n_content\",\"n_impressions\", \n",
    "                 \"n_likes\",\"n_quotes\", \"n_replies\", \n",
    "                 \"n_retweets\"]] = participants_tw[[\"n_content\",\"n_impressions\", \n",
    "                                                  \"n_likes\",\"n_quotes\",\n",
    "                                                  \"n_replies\", \"n_retweets\"]].fillna(0)\n",
    "    participants_tw.to_excel(\n",
    "        f\"{base}01-preprocessed/report/{datep}/twitter.xlsx\", index=False\n",
    "    )\n",
    "    summ_tw = participants_tw.groupby([\"treatment\"]).sum().reset_index()\n",
    "    summ_tw = summ_tw[[\"treatment\", \"content\"]]\n",
    "    summ_tw = summ_tw.rename({\"content\": \"percent_share\"}, axis=1)\n",
    "    summ_tw[\"percent_share\"] = (summ_tw[\"percent_share\"] / 38) * 100\n",
    "    summ_tw.to_excel(\n",
    "        f\"{base}01-preprocessed/report/{datep}/summary_twitter.xlsx\", index=False\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    monitor_influencers()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
