{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from langdetect import DetectorFactory, detect\n",
    "DetectorFactory.seed = 0\n",
    "import re\n",
    "\n",
    "def get_path(country, week):\n",
    "    base = f'../../data/03-experiment/{country}/'\n",
    "    path_tw = base + f'treatment/followers/00-raw/tweets/{week}/'\n",
    "    rand = f'../../data/02-randomize/{country}/04-stratification/integrate/followers_randomized.parquet'\n",
    "    baseline = base + 'baseline/00-raw/followers/tweets/'\n",
    "    baseline2 = base + f'baseline/00-raw/followers/tweets_batch2/{week}/'\n",
    "    agg = base + f'treatment/followers/01-preprocess/'\n",
    "    agg_base = base + 'baseline/01-preprocess/followers/'\n",
    "    return path_tw, base, rand, baseline, baseline2, agg, agg_base\n",
    "\n",
    "df_vars = ['id', 'handle', 'author_id', 'created_at', 'text', 'lang', 'referenced_tweets',\n",
    "           'entities.urls','public_metrics.like_count', 'public_metrics.quote_count', \n",
    "           'public_metrics.reply_count', 'public_metrics.retweet_count']\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "from funcs import PreprocessTweets\n",
    "def compute_engagement_tw(engage):\n",
    "    engage[\"total_reactions\"] = engage[\n",
    "        [x for x in engage.columns if \"public_metrics\" in x]\n",
    "    ].sum(axis=\"columns\")\n",
    "    engage[\"total_comments\"] = engage[\n",
    "        [\"public_metrics.reply_count\", \"public_metrics.quote_count\"]\n",
    "    ].sum(axis=\"columns\")\n",
    "    engage.rename(\n",
    "        {\"public_metrics.retweet_count\": \"total_shares\"}, axis=1, inplace=True\n",
    "    )\n",
    "    return(engage)\n",
    "\n",
    "def lang_detect_na(tweet):\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "    except:\n",
    "        lang = 'NA'\n",
    "    return lang\n",
    "\n",
    "def compute_engagement_tw2(engage):\n",
    "    engage[\"total_reactions\"] = engage[\n",
    "        [\"reply_count\", \"quote_count\", \"like_count\", \n",
    "         'retweet_count']].sum(axis=\"columns\")\n",
    "    engage[\"total_comments\"] = engage[\n",
    "        [\"reply_count\", \n",
    "         \"quote_count\"]].sum(axis=\"columns\")\n",
    "    engage.rename(\n",
    "        {\"retweet_count\": \"total_shares\"}, axis=1, inplace=True)\n",
    "    return(engage)\n",
    "\n",
    "def has_items_tw(df):\n",
    "\n",
    "    has_items = PreprocessTweets(df).preprocess()\n",
    "    has_items.drop(['entities.urls', 'description', 'display_url',\n",
    "    'end', 'expanded_url', 'images', 'media_key', 'start', \n",
    "    'status', 'title', 'unwound_url', 'url'], axis=1, inplace=True)\n",
    "    \n",
    "    return(has_items)\n",
    "\n",
    "def has_text(string):\n",
    "    return bool(re.search(r'[a-zA-Z]', string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19557"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:56<08:32, 56.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2000 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:48<07:12, 54.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4000 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:31<08:53, 76.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6000 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [05:02<08:12, 82.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8000 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [06:28<06:57, 83.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10000 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [07:48<05:29, 82.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12000 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [09:20<04:16, 85.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 14000 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [10:54<02:56, 88.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 16000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [12:22<01:28, 88.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [13:24<00:00, 80.50s/it]\n"
     ]
    }
   ],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'april')\n",
    "onlyfiles = [f for f in listdir(baseline2) if isfile(join(baseline2, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)\n",
    "\n",
    "for i in tqdm(range(0, 10)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 2000*i \n",
    "    b = 2000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{baseline2}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] = df_final['text'].astype(str)\n",
    "    df_final['has_text'] = df_final['text'].apply(has_text)\n",
    "    # Check if the 'Name' column contains words by splitting and checking the resulting list length\n",
    "    df_final['has_words'] = df_final['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg_base}intermediate/baseline_batch2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [01:12<15:47, 72.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2000 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [02:21<14:01, 70.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4000 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [03:17<11:43, 63.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6000 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [04:21<10:37, 63.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8000 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [05:25<09:36, 64.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10000 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [06:37<08:53, 66.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12000 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [07:53<08:08, 69.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 14000 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [09:36<08:02, 80.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 16000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [11:13<07:07, 85.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [12:09<05:05, 76.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20000 22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [13:03<03:28, 69.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 22000 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [14:03<02:12, 66.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 24000 26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [14:59<01:03, 63.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 26000 28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [15:26<00:00, 66.18s/it]\n"
     ]
    }
   ],
   "source": [
    "country = 'KE'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'april')\n",
    "onlyfiles = [f for f in listdir(baseline2) if isfile(join(baseline2, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)\n",
    "\n",
    "for i in tqdm(range(0, 14)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 2000*i \n",
    "    b = 2000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{baseline2}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] = df_final['text'].astype(str)\n",
    "    df_final['has_text'] = df_final['text'].apply(has_text)\n",
    "    # Check if the 'Name' column contains words by splitting and checking the resulting list length\n",
    "    df_final['has_words'] = df_final['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg_base}intermediate/baseline_batch2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'march')\n",
    "onlyfiles = [f for f in listdir(baseline2) if isfile(join(baseline2, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/03-experiment/SA/treatment/followers/01-preprocess/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [00:51<11:12, 51.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2000 4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [01:49<11:03, 55.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 4000 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [02:46<10:16, 56.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 6000 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [03:49<09:48, 58.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 8000 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [05:02<09:33, 63.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 10000 12000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [06:15<08:56, 67.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 12000 14000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [07:30<08:07, 69.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 14000 16000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [09:06<07:49, 78.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 16000 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [10:32<06:43, 80.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 18000 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [11:52<05:21, 80.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20000 22000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [12:54<03:43, 74.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 22000 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [13:57<02:22, 71.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 24000 26000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [15:07<01:10, 70.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 26000 28000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [16:14<00:00, 69.59s/it]\n"
     ]
    }
   ],
   "source": [
    "country = 'KE'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'march')\n",
    "onlyfiles = [f for f in listdir(baseline2) if isfile(join(baseline2, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)\n",
    "\n",
    "for i in tqdm(range(0, 14)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 2000*i \n",
    "    b = 2000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{baseline2}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] = df_final['text'].astype(str)\n",
    "    df_final['has_text'] = df_final['text'].apply(has_text)\n",
    "    # Check if the 'Name' column contains words by splitting and checking the resulting list length\n",
    "    df_final['has_words'] = df_final['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg_base}intermediate/baseline2_batch2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'march')\n",
    "onlyfiles = [f for f in listdir(baseline2) if isfile(join(baseline2, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "len(onlyfiles)\n",
    "\n",
    "for i in tqdm(range(0, 14)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 2000*i \n",
    "    b = 2000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{baseline2}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] = df_final['text'].astype(str)\n",
    "    df_final['has_text'] = df_final['text'].apply(has_text)\n",
    "    # Check if the 'Name' column contains words by splitting and checking the resulting list length\n",
    "    df_final['has_words'] = df_final['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/baseline2_batch2_{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Endline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19178"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country = 'KE'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'april')\n",
    "path_tw1 = f'../../data/03-experiment/{country}/treatment/followers/00-raw/tweets_batch2/may/'\n",
    "onlyfiles = [f for f in listdir(path_tw1) if isfile(join(path_tw1, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "\n",
    "for i in tqdm(range(0, 9)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{path_tw1}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang2'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/may_batch2{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'SA'\n",
    "path_tw, base, rand, baseline, baseline2, agg, agg_base = get_path(country, 'april')\n",
    "path_tw1 = f'../../data/03-experiment/{country}/treatment/followers/00-raw/tweets_batch2/may/'\n",
    "onlyfiles = [f for f in listdir(path_tw1) if isfile(join(path_tw1, f))]\n",
    "onlyfiles = [f.replace('.parquet', '') for f in onlyfiles]\n",
    "onlyfiles.sort()\n",
    "\n",
    "for i in tqdm(range(0, 7)):\n",
    "    df_final = pd.DataFrame()\n",
    "    a = 3000*i \n",
    "    b = 3000*(i+1)\n",
    "    #time.sleep(5)\n",
    "    for e in onlyfiles[a:b]:\n",
    "        df = pd.read_parquet(f'{path_tw1}{e}.parquet')\n",
    "        df_final = pd.concat([df_final, df])\n",
    "    df_final = df_final.reset_index(drop=True)\n",
    "    df_final['text'] =df_final['text'].astype(str)\n",
    "    df_final['lang2'] = df_final['text'].apply(lang_detect_na)\n",
    "    df_final = compute_engagement_tw2(df_final)\n",
    "    print(i, a, b)\n",
    "    df_final.to_parquet(f'{agg}intermediate/may_batch2{i}.parquet.gzip', \n",
    "                        compression = 'gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
