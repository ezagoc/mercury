{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e53fcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from langdetect import DetectorFactory, detect\n",
    "DetectorFactory.seed = 0\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "def get_path(country, week):\n",
    "    base = f'../../../data/03-experiment/{country}/'\n",
    "    agg = base + f'treatment/followers/00-raw/'\n",
    "    agg_p = base + f'treatment/followers/01-preprocess/'\n",
    "    return base, agg, agg_p\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "\n",
    "def lang_detect_na(tweet):\n",
    "    from langdetect import DetectorFactory, detect\n",
    "    DetectorFactory.seed = 0\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "    except:\n",
    "        lang = None\n",
    "    return lang\n",
    "\n",
    "def has_text(string):\n",
    "    return bool(re.search(r'[a-zA-Z]', string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40512d",
   "metadata": {},
   "source": [
    "Use this code to update the overall file for prediction, it takes the last aggregated one and just keeps the posts that havent been preprocessed yet. After this, take the output file and predict on google colab, save it back and aggregate using the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad748874",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_before = 'correct_cases_final.parquet.gzip'\n",
    "\n",
    "# Update with new date given by Joaquin\n",
    "file_new = 'correct_followers_09_06_25.parquet'\n",
    "file_final = 'correct_cases_09_06_25.parquet.gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c60222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n",
      "On to Lang Detection, this might take a few hours, number of rows:\n",
      "2901677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3b09a32c634cd09b27c427c7ef2b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=483613), Label(value='0 / 483613')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On to Lang Detection, this might take a few hours, number of rows:\n",
      "1833483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1e0819a5014fc7b6d50d1f2e3c03f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=305581), Label(value='0 / 305581')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pandarallel.initialize(progress_bar=True, nb_workers=6)  \n",
    "for country in ['KE', 'SA']:\n",
    "    base, agg, agg_p = get_path(country, 'april')\n",
    "    old = pd.read_parquet(f'{agg_p}/{file_before}')\n",
    "    old = old[['id']].drop_duplicates(['id']).reset_index(drop = True)\n",
    "    old['dumdum'] = 1\n",
    "    old= old[['id', 'dumdum']]\n",
    "    new = pd.read_parquet(f'../../../../../manual_scraper/data/correct_cases/{country}/{file_new}')\n",
    "    new['TimeStamp'] = pd.to_datetime(new['TimeStamp'])\n",
    "    new['id'] = new['follower_handle'].astype(str) + new['TimeStamp'].astype(str)\n",
    "    new = new.merge(old, on = ['id'], how = 'left')\n",
    "    new = new[new['dumdum'].isnull()]\n",
    "    new = new.drop_duplicates('id').reset_index(drop = True)\n",
    "    new['text'] = new['text'].astype(str)\n",
    "    new['has_text'] = new['text'].apply(has_text)\n",
    "    new['has_words'] = new['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    print(f'On to Lang Detection, this might take a few hours, number of rows:')\n",
    "    print(f'{len(new)}')\n",
    "    new['lang2'] = new['text'].parallel_apply(lang_detect_na)\n",
    "    new.to_parquet(f'{agg_p}/{file_final}', \n",
    "                   compression = 'gzip')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
