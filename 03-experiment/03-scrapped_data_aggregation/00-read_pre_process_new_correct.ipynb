{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e53fcff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm\n",
    "from langdetect import DetectorFactory, detect\n",
    "DetectorFactory.seed = 0\n",
    "import re\n",
    "\n",
    "def get_path(country, week):\n",
    "    base = f'../../../data/03-experiment/{country}/'\n",
    "    agg = base + f'treatment/followers/00-raw/'\n",
    "    agg_p = base + f'treatment/followers/01-preprocess/'\n",
    "    return base, agg, agg_p\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../../src/utils')\n",
    "\n",
    "def lang_detect_na(tweet):\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "    except:\n",
    "        lang = 'NA'\n",
    "    return lang\n",
    "\n",
    "def has_text(string):\n",
    "    return bool(re.search(r'[a-zA-Z]', string))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f40512d",
   "metadata": {},
   "source": [
    "Use this code to update the overall file for prediction, it takes the last aggregated one and just keeps the posts that havent been preprocessed yet. After this, take the output file and predict on google colab, save it back and aggregate using the following code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad748874",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_before = 'correct_cases_final.parquet.gzip'\n",
    "file_new = 'correct_followers_02_08_24.parquet'\n",
    "file_final = 'correct_cases_02_08_24.parquet.gzip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c60222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On to Lang Detection, this might take a few hours\n",
      "On to Lang Detection, this might take a few hours\n"
     ]
    }
   ],
   "source": [
    "for country in ['KE', 'SA']:\n",
    "    base, agg, agg_p = get_path(country, 'april')\n",
    "    old = pd.read_parquet(f'{agg_p}/{file_before}')\n",
    "    old = old[['id']].drop_duplicates(['id']).reset_index(drop = True)\n",
    "    old['dumdum'] = 1\n",
    "    old= old[['id', 'dumdum']]\n",
    "    new = pd.read_parquet(f'../../../../../manual_scraper/data/correct_cases/{country}/{file_new}')\n",
    "    new['TimeStamp'] = pd.to_datetime(new['TimeStamp'])\n",
    "    new['id'] = new['follower_handle'].astype(str) + new['TimeStamp'].astype(str)\n",
    "    new = new.merge(old, on = ['id'], how = 'left')\n",
    "    new = new[new['dumdum'].isnull()]\n",
    "    new = new.drop_duplicates('id').reset_index(drop = True)\n",
    "    new['text'] = new['text'].astype(str)\n",
    "    new['has_text'] = new['text'].apply(has_text)\n",
    "    new['has_words'] = new['text'].str.split().apply(lambda x: len(x) > 0)\n",
    "    print('On to Lang Detection, this might take a few hours')\n",
    "    new['lang2'] = new['text'].apply(lang_detect_na)\n",
    "    new.to_parquet(f'{agg_p}/{file_final}', \n",
    "                   compression = 'gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8122dc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
