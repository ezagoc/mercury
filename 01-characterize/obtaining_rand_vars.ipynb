{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "483b0cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9eee2ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pattern():\n",
    "\n",
    "    \"\"\"Patterns to search\"\"\"\n",
    "\n",
    "    news = pd.read_excel(f'../../../../social_influencers_africa/data/newspapers/urls_newspapers_za_ke_validated.xlsx')\n",
    "    news = news[news['valid'] == True]\n",
    "    newspapers = list(news[~news['url_site'].isna()]['url_site'])\n",
    "    newspapers = [url.split('://')[1] for url in newspapers]\n",
    "    patternnews = '|'.join([f'(?i){url}' for url in newspapers])\n",
    "    factcheckers = list(news[news.factchecker == 1]['url_site'])\n",
    "    factcheckers = [url.split('://')[1] for url in factcheckers]\n",
    "    patternfactchecks = '|'.join([f'(?i){url}' for url in factcheckers])\n",
    "\n",
    "    return patternnews, patternfactchecks\n",
    "\n",
    "\n",
    "def journalistic_content(df, identifier, link_column):\n",
    "\n",
    "    \"\"\"Create dummies depending if post if content related to newsarticles of factchecks\"\"\"\n",
    "\n",
    "    patternnews, patternfactchecks = create_pattern()\n",
    "\n",
    "    # Create dummy if shared newsarticles\n",
    "    df01 = df[~df[link_column].isna()]\n",
    "    df02 = df[df[link_column].isna()]\n",
    "    df02['link_newsarticle'] = 0\n",
    "    dfnews = df01[df01[link_column].str.contains(patternnews)]\n",
    "    dfnews['link_newsarticle'] = 1\n",
    "    df = pd.concat([dfnews, df02, df01], ignore_index=True).reset_index(drop=True)\n",
    "    df['link_newsarticle'] = df['link_newsarticle'].fillna(0)\n",
    "    df = df.drop_duplicates(subset=[identifier], keep='first')\n",
    "\n",
    "    # Create dummy if shared fact checks\n",
    "    df01 = df[~df[link_column].isna()]\n",
    "    df02 = df[df[link_column].isna()]\n",
    "    df02['link_factcheck'] = 0\n",
    "    dffacts = df01[df01[link_column].str.contains(patternfactchecks)]\n",
    "    dffacts['link_factcheck'] = 1\n",
    "    df = pd.concat([dffacts, df02, df01], ignore_index=True).reset_index(drop=True)\n",
    "    df['link_factcheck'] = df['link_factcheck'].fillna(0)\n",
    "    df = df.drop_duplicates(subset=[identifier], keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def expand_column(df, column):\n",
    "\n",
    "    df0 = df[~df[column].isna()]\n",
    "    df0 = df0.reset_index(drop=True)\n",
    "    dfi = pd.DataFrame(df0[column].tolist())\n",
    "    dfi = dfi[0].apply(pd.Series)\n",
    "    df0 = pd.concat([df0, dfi], axis=1)\n",
    "    df0 = df0.reset_index(drop=True)\n",
    "    df1 = df[df[column].isna()].reset_index(drop=True)\n",
    "    df = pd.concat([df0, df1], ignore_index=True)\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "089ef4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'SA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "40c253dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-118-69507e9a992b>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df02['link_newsarticle'] = 0\n",
      "<ipython-input-118-69507e9a992b>:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfnews['link_newsarticle'] = 1\n",
      "<ipython-input-118-69507e9a992b>:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df02['link_factcheck'] = 0\n",
      "<ipython-input-118-69507e9a992b>:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dffacts['link_factcheck'] = 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    dft = pd.read_parquet(f'../../data/03-experiment/{country}/baseline/01-preprocess/influencers/tweets_batch2.parquet')\n",
    "    dft = dft.join(pd.json_normalize(dft['entities.urls']))\n",
    "    dft = journalistic_content(dft, 'id', 'expanded_url')\n",
    "    dft.to_parquet(\n",
    "        f'../../data/03-experiment/{country}/baseline/01-preprocess/influencers/tweets_na_batch2.parquet.gzip',\n",
    "        compression='gzip'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c99f9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_column(colname, idcol):\n",
    "        \"\"\"Summmarize column\"\"\"\n",
    "\n",
    "        descrip = ['min', 'mean', 'max', 'sum', 'std']\n",
    "        engsum = df.groupby(idcol).agg({colname: descrip})\n",
    "        engsum.columns = [\n",
    "            f'{colname}_min',\n",
    "            f'{colname}_mean',\n",
    "            f'{colname}_max',\n",
    "            f'{colname}_sum',\n",
    "            f'{colname}_std']\n",
    "        return(engsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "88a6aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-125-0d59a2445e9e>:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  engage.columns = engage.columns.str.replace(r'public_metrics.', '')\n"
     ]
    }
   ],
   "source": [
    "        df = pd.read_parquet(\n",
    "            f'../../data/03-experiment/{country}/baseline/01-preprocess/influencers/tweets_na_batch2.parquet.gzip'\n",
    "            )\n",
    "        cols = [col for col in df.columns if 'public_metrics' in col]\n",
    "        idcol = 'author_id'\n",
    "        df[idcol] = df[idcol].astype(str)\n",
    "        colname = [idcol]*len(cols)\n",
    "        engage = pd.concat(\n",
    "            list(map(summ_column, cols, colname)),\n",
    "            axis=1\n",
    "            ).reset_index()\n",
    "        engage['interactions_count_sum'] = engage[[\n",
    "            col for col in engage.columns if 'sum' in col]].sum(axis=1)\n",
    "        engage.columns = engage.columns.str.replace(r'public_metrics.', '')\n",
    "        users = pd.read_parquet(\n",
    "            f'../../data/01-characterize/influencers/{country}/confirmed_influencers_batch2.parquet'\n",
    "            )\n",
    "        rename_users = {'id': 'author_id'}\n",
    "        users = users.rename(columns=rename_users)\n",
    "        users['author_id'] = users['author_id'].astype(str)\n",
    "        tup_stats = {\n",
    "            'link_factcheck': 'sum',\n",
    "            'link_newsarticle': 'sum',\n",
    "            'id': 'count'\n",
    "            }\n",
    "        stats_news = df.groupby(idcol, as_index=False).agg(tup_stats)\n",
    "        rename_stats = {\n",
    "            'link_factcheck': 'n_tweets.fc',\n",
    "            'link_newsarticle': 'n_tweets.na',\n",
    "            'id': 'n_tweets'\n",
    "        }\n",
    "        stats_news = stats_news.rename(columns=rename_stats)\n",
    "        users = stats_news.merge(users, on=idcol, how='left')\n",
    "        users = users.merge(engage, on=idcol, how='left')\n",
    "        #users = users[(users['n_tweets.fc'] != 0) | (users['n_tweets.na'] != 0)].reset_index(drop=True)\n",
    "        #users.to_parquet(\n",
    "          #  f'../../data/03-experiment/{country}/baseline/01-preprocess/influencers/accounts_sa.parquet'\n",
    "           # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4af1a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8e9893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coun = 'ke'\n",
    "\n",
    "users_ke_1=pd.read_parquet(f'../../../../social_influencers_africa/data/twitter/1-candidates/possible_users/users_{coun}_filtered.parquet')\n",
    "\n",
    "users_ke_2=pd.read_parquet(f'../../../../social_influencers_africa/data/twitter/1-candidates/possible_users/users_{coun}_filtered_additional.parquet')\n",
    "users_ke = pd.concat([users_ke_1, users_ke_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b6a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6a0a2dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ke.drop_duplicates('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b4939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ke.rename(columns={'id':'author_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9561a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ke['author_id'] = users_ke['author_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f84a43e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.merge(users,users_ke,on='author_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "08774bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/joaquinbarrutia/miniforge3/envs/env_tf/lib/python3.9/site-packages/tweetple/TweetPle.py:61: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_stats = df_stats.append(\n",
      "1it [00:01,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "import tweetple\n",
    "\n",
    "from tweetple import TweetPle\n",
    "\n",
    "# Bearer token accesible via Twitter Developer Academic Research Track\n",
    "bearer_token='AAAAAAAAAAAAAAAAAAAAAFpgZAEAAAAAbJS59UWzipi32ixd7LHtXov9olo%3D7gxD8Afshgj4munMXHLU08jzRdTpsAh4RZqq7VBofq1wAvkx1T'\n",
    "\n",
    "# List of handle ids\n",
    "ids = list(data[data['username'].isnull()].author_id)\n",
    "\n",
    "\n",
    "missing_info = TweetPle.TweepleStreamer(ids, bearer_token).user_lookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9e183b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_info = data[data['username'].isnull()]\n",
    "full_info = data[~data['username'].isnull()]\n",
    "\n",
    "no_info = no_info[['author_id', 'n_tweets.fc', 'n_tweets.na', 'n_tweets', 'handle',\n",
    "       'impression_count_min', 'impression_count_mean', 'impression_count_max',\n",
    "       'impression_count_sum', 'impression_count_std', 'like_count_min',\n",
    "       'like_count_mean', 'like_count_max', 'like_count_sum', 'like_count_std',\n",
    "       'quote_count_min', 'quote_count_mean', 'quote_count_max',\n",
    "       'quote_count_sum', 'quote_count_std', 'reply_count_min',\n",
    "       'reply_count_mean', 'reply_count_max', 'reply_count_sum',\n",
    "       'reply_count_std', 'retweet_count_min', 'retweet_count_mean',\n",
    "       'retweet_count_max', 'retweet_count_sum', 'retweet_count_std',\n",
    "       'interactions_count_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "18f49292",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_info = missing_info[['id','location', 'created_at', 'description',\n",
    "       'name', 'url', 'username', 'verified', 'public_metrics.followers_count',\n",
    "       'public_metrics.following_count', \n",
    "                 'public_metrics.listed_count', 'public_metrics.tweet_count']].rename(columns=\n",
    "                                                                                      {'id':'author_id',\n",
    "                                                                                      'public_metrics.followers_count':'followers_count',\n",
    "                                                                                      'public_metrics.listed_count':'listed_count',\n",
    "                                                                                      'public_metrics.tweet_count':'tweet_count',\n",
    "                                                                                      'public_metrics.following_count':'following_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b562412",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_info = pd.merge(no_info,missing_info,on='author_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d8533a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c2df87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([full_info, no_info], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4d132388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import datetime as dt\n",
    "\n",
    "today = date.today()\n",
    "today=pd.Timestamp(today).replace(tzinfo=dt.timezone.utc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e7047593",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime(data['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d44382e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['days_old_account'] = (today - data['created_at']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "76f853cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_parquet(\n",
    "            f'../../data/02-randomize/{country}/02-variables/variables_batch2.parquet'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "17196364",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_parquet(\n",
    "            f'../../data/02-randomize/SA/02-variables/variables_batch2.parquet'\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
